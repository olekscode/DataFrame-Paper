\documentclass[sigplan]{acmart}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}

%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[IWST'17]{International Workshop on Smalltalk Technologies}{ESUG 2017}{Maribor, Slovenia 4-8 September 2017} 
%\acmYear{1997}
\copyrightyear{2017}

%\acmPrice{15.00}

%\acmBadgeL[http://ctuning.org/ae/ppopp2016.html]{ae-logo}
%\acmBadgeR[http://ctuning.org/ae/ppopp2016.html]{ae-logo}



\usepackage[T1]{fontenc} %%%key to get copy and paste for the code!
\usepackage[utf8]{inputenc} %%% to support copy and paste with accents for frnehc stuff
%\usepackage{times}
%\usepackage[scaled=0.85]{helvet}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{xspace}
\usepackage{alltt}
\usepackage{latexsym}
\usepackage{url}            
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{enumerate}
%\usepackage{cite}
% \usepackage[pdftex,colorlinks=true,pdfstartview=FitV,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
%\usepackage{xspace}
%\newcommand*{\eg}{e.g.\@\xspace}
%\newcommand*{\ie}{i.e.\@\xspace}

\makeatletter
\newcommand*{\etc}{%
    \@ifnextchar{.}%
        {etc}%
        {etc.\@\xspace}%
}
\makeatother

\usepackage{float}
% \usepackage{xcolor}
% \usepackage{listings}
% \usepackage{highlight}

%\usepackage{multicol}
\usepackage{url}
\usepackage{multicol}

% \usepackage
% [
%   left=20mm,
%   right=20mm,
%   top=30mm,
%   bottom=30mm,
%   bindingoffset=10mm
%   % use vmargin=2cm to make vertical margins equal to 2cm.
%   % use hmargin=3cm to make horizontal margins equal to 3cm.
%   % use margin=3cm to make all margins  equal to 3cm.
% ] {geometry}

\input{macros}

%\setlength\parindent{0pt}

% \date{}

\input{smalltalkEnv}

\begin{document}
\title{Towards Exploratory Data Analysis for Pharo}
% \titlenote{Produces the permission block, and
%   copyright information}
% \subtitle{Extended Abstract}
% \subtitlenote{The full version of the author's guide is available as
%   \texttt{acmart.pdf} document}

\author{Oleksandr Zaytsev}
%\authornote{Dr.~Trovato insisted his name be first.}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{Ukrainian Catholic University\\
  Faculty of Applied Sciences}
  % \streetaddress{P.O. Box 1212}
  \city{Lviv}
  \state{Ukraine}
  % \postcode{43017-6221}
}
\email{oleks@ucu.edu.ua}

\author{Nick Papoulias}
%\authornote{Dr.~Trovato insisted his name be first.}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{
IRD, Sorbonne Universités, UPMC Univ Paris 06, Unité Mixte Internationale de Modélisation Mathématique et Informatiques des Systèmes Complexes (UMMISCO), Bondy, France}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin}
  % \state{Ohio}
  % \postcode{43017-6221}
}
\email{npapoylias@gmail.com}


\author{Serge Stinckwich}
%\authornote{Dr.~Trovato insisted his name be first.}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{
IRD, Sorbonne Universités, UPMC Univ Paris 06, Unité Mixte Internationale de Modélisation Mathématique et Informatiques des Systèmes Complexes (UMMISCO), Bondy, France\\Université de Caen Normandie, Caen, France}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin}
  % \state{Ohio}
  % \postcode{43017-6221}
}
\email{serge.stinckwich@ird.fr}

%\title{Title that Describes the Contribution that Solves a Problem}
%\author{Oleksandr Zaytsev}
%\date{\today}
%\maketitle
\renewcommand{\shortauthors}{O. Zaytsev et al.}
\begin{abstract}
% In this context...
% We consider this problem P...
% P is a problem because...
% We propose this solution...
% Our solution solves P in such and such way.

Data analysis and visualizations techniques (such as split-apply-combine) make extensive use of associative tabular data-structures that are cumbersome to use with common aggregation APIs (for arrays, lists or dictionaries). In these cases a fluent API for querying associative tabular data (like the ones provided by Pandas, Mathematica or LINQ) is more appropriate for interactive exploration environments. In Smalltalk despite the fact that many important analysis tools are already present (for \eg in the PolyMath library), we are still missing this essential part of the data science toolkit. These specialized data structures for tabular datasets can provide us with a simple and powerful API for summarizing, cleaning, and manipulating a wealth of data-sources that are currently cumbersome to use. In this paper we introduce the \texttt{DataFrame} and \texttt{DataSeries} collections - that are specifically designed for working with structured data. We demonstrate how these tools can be used for descriptive statistics and Exploratory Data Analysis (EDA) - the critical first step of data analysis which allows us to get the summary of a dataset, detect mistakes, determine the relations, and select the appropriate model for further confirmatory analysis. We then detail the implementation trade-offs that we are currently facing in our implementation for Pharo and discuss future perspectives.
\end{abstract}


\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Tabular Data-structures, Fluent APIs, Exploratory Data Analysis,
Live Environments}

\maketitle

%\begin{multicols}{2}
\section{Introduction}
\label{sec:intro}

% Context
%
% Problem
%
% Known tracks for \sd{solutions}
% 	here you want to show that you are not an idiot not knowing what have been around
%
% What our solution is \ct{Set} and \ct{OrderedCollection} (so that the reader knows where the paper is going)
%
% Contribution of the paper

The simplicity and power of Smalltalk combined with the live environment of Pharo creates a productive combination for data analysis. 
% The advanced debugging and inspecting tools together with the library for agile visualizations allow us to communicate and play with every object in our system. This includes all the logical components of both data and the algorithm.
Provided the proper tools and open source libraries for machine learning, statistics, and optimization, Pharo can become both a powerful tool for professional data analysts, and a simple environment for everyone who wants to experiment with a simple dataset. Many important tools and algorithms are already implemented in libraries such as PolyMath\footnote{\url{https://github.com/PolyMathOrg/PolyMath}}, but we are still missing essential data-structures and a fluent API for advanced data-analysis techniques. To overcome this problem we introduce in this paper the \texttt{DataFrame} and \texttt{DataSeries} collections for working with structured data, and through several examples demonstrate how these tools can be used for descriptive statistics and exploratory data analysis allowing us to get the summaries of datasets, detect mistakes, determine relations, and select the appropriate models for further analysis.

Our work is motivated by popular data-analysis techniques (such as split-apply-combine \cite{wickham2011split} and collection pipelines \cite{Fowler15}) as well as from dedicated data-structure and APIs for data-analysis environments for Python \cite{McKinney}, R \cite{team2000r}, Mathematica \cite{wolfram1999mathematica} and the .Net platform (through the LINQ embedded query language \cite{meijer2006linq}). Nevertheless our primary inspiration is drawn from Smalltalk itself and can be traced back to the extensive coverage of the Smalltalk-80 book on Collections \cite{Goldberg} (with over four chapters dedicated to the usage and design of the Collection hierarchy), as well as from more recent work from our community that refined the implementation of collections using traits \cite{black2003applying, bourgois2010bloc, scharli2003traits} and promises \cite{Alcocer16}.

To facilitate our readers, we should note here that all figures in this paper are DataFrame visualizations that are created with Roassal2\footnote{\url{http://agilevisualization.com/}} and can be reproduced by the steps described in Section \ref{sec:dataframe}.
The rest of this paper is structured as follows: Section \ref{sec:eda} provides a brief introduction into Exploratory Data Analysis (EDA), answering the following question: \textit{What is EDA good for and how to do it right?}. Then Section \ref{sec:dataframe} describes the new DataSeries and DataFrame Collections for structured data. Section \ref{sec:iris-eda} gives a step-by-step example of how to perform EDA on the well-known Iris dataset using the new collections and API.
Finally Section \ref{sec:conclusion} concludes the paper and discusses implementation trade-offs as well as future perspectives.


% \section{Problem Description}
% \label{sec:problem}

% Context, exposed with the \textbf{most precise terms possible} (don't open
% unwanted doors for the reader)
%
% Probably set the vocabulary before to cut any misinterpretation
%
% Constraints that influenced the solution (because the solution is not
% universal) \emph{e.g.} our requirements for a solution, possibly not all
% satisfied. They should be sound and believable. Analysis of the criteria.
% Imagine that you are another guy having this problem do the constraint
% matches yours so that you could apply the solution
%
% Problem
%
% Factual solution tracks, to position...
% Our solution in a nutshell.

\section{Exploratory Data Analysis}
\label{sec:eda}
\textit{Exploratory data analysis} is an approach for analyzing datasets to summarize their main characteristics. It allows us to make some sense of the data by visualizing it and exploring its statistical properties.
According to \cite{Seltman}, "any method of looking at data that does not include formal statistical modeling and inference falls under the term exploratory data analysis". It helps us to select the model that we will be fitting to the data during the following steps of confirmatory data analysis.

EDA can be particularly useful for uncovering the underlying structure of a dataset, detecting outliers and anomalies, determining relationships among the explanatory variables, assessing the direction and rough size of relationships between explanatory and outcome variables, and selecting appropriate models for further analysis\cite{eStats}.

Howard J. Seltman \cite{Seltman} classifies the techniques of exploratory data analysis into four categories: univariate non-graphical (Sub-section \ref{sec:uni-non}), univariate graphical (Sub-section \ref{sec:uni-graph}), multivariate non-graphical (Sub-section \ref{sec:multi-non}) and multivariate graphical (Sub-section \ref{sec:multi-graph}).

\section{DataSeries and DataFrame}
\label{sec:dataframe}
DataFrame and DataSeries\footnote{The source code of DataFrame, the tutorial, and the instructions for loading it into a Pharo image can be found here: \url{https://github.com/PolyMathOrg/DataFrame}} are the high-level data structures with which we intend to make data analysis in Pharo fast and easy.%\cite{McKinney}

\textbf{A DataSeries} can be seen as an sequenceable collection that combines the properties of an Array and a Dictionary, while extending the functionality of both. Every DataSeries has a name and contains an array of data mapped to a corresponding array of keys (that are used as index values).

The easiest way of to create a series is by converting an array: 

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
series := #(a b c) asDataSeries.
\end{lstlisting}

The keys will be automatically set to the numeric sequence of the array indexes, which can be described as an interval (1 to: n), where n is the size of array. The name of the series at this point will remain empty. Both the name and the keys of a DataSeries can be changed later, as follows:

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
series name: 'letters'.
series keys: #(r1 r2 r3).
\end{lstlisting}

\textbf{A DataFrame} is a tabular data structure that can be seen as an ordered collection of columns. It works like a spreadsheet or a relational database with one row per subject and one column for each subject identifier, outcome variable, explanatory variable \etc. A DataFrame has both row and column indices which can be changed if needed. The important feature of a DataFrame is that whenever we ask for a specific row or column, it responds with a DataSeries object that preserves the same indexing.

A simple DataFrame can be created from an array of rows or columns.

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
df:=DataFrame 
     rows: #((John 25 true)(Jane 21 false)).
df:=DataFrame 
     columns: #((John Jane)(25 21)(true false)).
\end{lstlisting}

Those two lines produce exactly the same DataFrame. Once again, the names (key values) of both rows and columns were not explicitly specified, so by default they will set to numerical indices: \#(1 2) for rows and \#(1 2 3) for columns. These names can be changed later:

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
df columnNames: #(Name Age IsMarried).
\end{lstlisting}

\subsection*{Discussion: DataFrame Internals}
\label{sec:internals}

Before continuing with a detailed example on how to use these classes for data exploration, we would like to share our thoughts on their implementation. 

Our goal for these classes is to fully encapsulate their internal structure so that it is both easy to experiment with optimized  representations, but also allow for a polymorphic fluent API that can be implemented separately without breaking existing code. 

This is why we are evolving the API and the implementation according to concrete needs we are facing while analyzing specific datasets (like the Iris dataset that we will shortly discuss). We are currently in the fifth iteration of the model, since there are many trade-offs to consider, between for \eg using inheritance, instance composition and/or trait composition for the implementation. Even inheriting directly for \eg from Array, Dictionary or OrderedDictionary comes with its own set of trade-offs. In the future we might consider reusing the internal structure of the pandas DataFrame in which the data is stored as one or more two-dimensional blocks rather than a collection of one-dimensional arrays \cite{McKinney}. 

% Finally functionalities such as the head/tail protocol or the protocol allowing us to change the keys of a data structure are all currently implemented as traits.
In future iterations, we expect to extend the use of traits to include the data visualization and aggregation protocols, that should be shared among the DataSeries and DataFrame classes. 

% Internally DataSeries and DataFrame are implemented as subclasses of OrderedDictionary. This way they support the key/value/association interface of a Dictionary and preserves the key insertion order. This is the fourth iteration of a model. Previously the data structures were implemented as subclasses of Dictionary or Array. Both approaches have proved to be ineffective, because an Array can not have variable size and Dictionary stores its elements in an unordered manner and uses the hashing technique to locate them\cite{Goldberg}. The model is evolving according to the needs, so while this implementation works fine on small datasets (like Iris or Housing data), it might prove to be useless when dealing with massive datasets. For that reason in future iterations we might consider reusing the internal structure of pandas DataFrame in which the data is stored as one or more two-dimensional blocks rather than a collection of one-dimensional arrays\cite{McKinney}.
% %
% % Another important thing about the implementation of DataFrame collections is the use of composition instead of inheritance. The problem is that DataSeries (one-dimensional) and DataFrame (two-dimensional) are classes of different nature that may require the entirely different intertal implementation. Nevertheless, we want them to be polymorphic and respond to the same messages (many of these messges will also have the same implementation). One way of achieving polymorphic behavior and code reuse without using inheritance is the composition reuse principle. In Pharo this can be done using the Traits instrument.
%
% At this point the head/tail functionality and the method that allows us to change keys of a data structure are implemented as traits. In future iterations, the data visualization, aggregation, and many other maethods that should be shared by DataSeries and DataFrame are also likely to be implemented as traits.

% Here is an example of how both DataSeries and DataFrame respond to a \texttt{head} message: "take first 5 elements from a collection, construct a collection of the same type with these elements, and return it". For a DataFrame these 5 elements will be rows of data (regardless of the internal implementation, the \texttt{at:} message should return a row as a single observation and a logical unit of a DataFrame). For DataSeries, these will be the atomic values.

%\section{Proposed Solution}%
\section{Exploring Iris Dataset}
\label{sec:iris-eda}
%
% Free form, variable number of sections, technical details.
%
% But in general do not mix solution and discussions/possible variation
% let that for discussion

In order to exemplify a real use-case of our library, we will start by loading the Iris dataset\footnote{\url{https://en.wikipedia.org/wiki/Iris_flower_data_set}} from a CSV file.

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
data := DataFrame fromCsv: '/path/to/iris.csv'.
\end{lstlisting}

DataFrame comes with a built-in collection of datasets that are widely used as examples for data analysis and machine learning problems. Iris is among them, so here is an alternative way of loading it

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
data := DataFrame loadIris.
\end{lstlisting}

To make sure that the data was loaded and to take a quick look on it, we can print its head (first 5 rows) or tail (last 5 rows). It is also possible to specify the number of rows that must be printed. As it was mentioned in \ref{sec:internals}, the same messages are supported by objects of the DataSeries class. This means that we can also look at a head or tail of a specific column. In the following example we ask for the first 5 and the last 3 rows of a DataFrame. Then we ask for the first and the last 5 elements of a \texttt{sepal\_length} column (which is a DataSeries).

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
data head.
data tail: 3.
(data column: #sepal_length) head.
(data column: #sepal_length) tail.
\end{lstlisting}

The first two messages were sent to a DataFrame. It will respond with another DataFrame object containing the requested rows. Here is the example output of the \texttt{data head} message.

\begin{lstlisting}[basicstyle=\small, belowskip=1em, aboveskip=1em,]
1 | 5.1  3.5  1.4  0.2  #setosa
2 | 4.9    3  1.4  0.2  #setosa
3 | 4.7  3.2  1.3  0.2  #setosa
4 | 4.6  3.1  1.5  0.2  #setosa
5 |   5  3.6  1.4  0.2  #setosa
\end{lstlisting}

When asked for a column named \texttt{sepal\_length}, DataFrame responds with a DataSeries object (how this object is constructed depends on the implementation and should not concern the user). Therefore, the second two messages are sent to a DataSeries which responds with another DataSeries containing all the requested elements.
The output of \texttt{(data column: \#sepal\_length) head} should look like this:

\begin{lstlisting}[basicstyle=\small]
  | sepal_length
--+-------------
1 |          5.1
2 |          4.9
3 |          4.7
4 |          4.6
5 |            5
\end{lstlisting}

First line holds the name of a series. The numbers in the following five lines are the key-value pairs. It is important that whenever a row or column is extracted from a DataFrame, it remembers all the key values (they are also called indices). So, for example, if we ask a DataFrame for its first row

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
data row: 1.
\end{lstlisting}

The DataSeries received as a response will store the column name of each value in that row as a corresponding key. And the key of the row (1 is a row name or a key of the first row in our DataFrame, not the numeric index like in Array) will be stored as the name of a DataSeries.

\begin{lstlisting}[basicstyle=\small]
             |   1
-------------+----        
sepal_length | 5.1
sepal_width  | 4.9
petal_length | 4.7 
petal_width  | 4.6
species      |   5 
\end{lstlisting}

\subsection{Univariate non-graphical EDA}
\label{sec:uni-non}

DataFrame can be viewed as a collection of statistical variables (columns). As demonstrated in \ref{sec:iris-eda}, we can ask DataFrame for a single column by its name. Since the rows and columns of a DataFrame are ordered, we can also access a column by its number. The following two lines return the same \texttt{sepal\_width} column:

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
data column: #sepal_width.
data columnAt: 2.
\end{lstlisting}

The best univariate non-graphical EDA technique for categorical data is a simple tabulation of frequencies for each category\cite{Seltman}. Let's look at a \texttt{species} column. As we can see by printing its unique values, \texttt{species} is a categorical variable with 3 categories: setosa, versicolor, and virginica

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
(data column: #species) unique.
\end{lstlisting}

Now we can tabulate this variable:

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
(data column: #species) frequencyTable.
\end{lstlisting}

The response will be the following DataFrame. The first column (row names) holds the unique categories, the second column - the number of occurences of each category, and the third one shows the fraction of data that belongs to each category. We can see that the sample data is evenly distributed among three categories.

\begin{lstlisting}[basicstyle=\small, belowskip=1em,]
            |  Count  Proportion  
------------+-------------------
    setosa  |     50       (1/3)  
versicolor  |     50       (1/3)  
 virginica  |     50       (1/3)
\end{lstlisting}

If the variable is quantitative, we can summarize it with the following statistics: \texttt{min, max, range, average, median, mode, stdev, variance}

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
(data column: #sepal_length) average. 
(data column: #petal_width) stdev.
\end{lstlisting}

\subsection{Univariate graphical EDA}
\label{sec:uni-graph}

The only graphical technique that can be used for a categorical variable is histogram - a barplot where the hight of each bar represents the count of cases for a range of values. Histogram is a graphical equivalent of \texttt{frequencyTable}, described in \ref{sec:uni-non}. If applied to a categorical variable, the height of each bar will be defined by the amount of data that belongs to a corresponding category.

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
(data column: #species) histogram.
(data column: #sepal_width) histogram.
\end{lstlisting}

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=0.75\linewidth]{species_bar}
  \caption{Barplot of a categorical species column}
  \end{center}
  \begin{center}
  \includegraphics[width=0.8\linewidth]{sepal_width_bar}
  \caption{Barplot of a quantitative sepal width column}
  \end{center}
\end{figure}

The important property of a histogram is the bin width - the number of cases that are summarized by a single bar. According to statisticians, like \cite{Seltman}, choosing a bin width is an iterative process. Usually it is chosen between 5 and 30, depending on the amount of data and the shape of distribution, which can be seen on a histogram.

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
(data column: #sepal_width) histogram: 5.
\end{lstlisting}

Quantitative variables can be visualized in many different ways: \texttt{plot, scatterplot, boxplot, histogram}. The following example demonstrates how we can ask a \texttt{petal\_width} column for a scatterplot of itself.
 
\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
(data column: #petal_width) scatterplot.
\end{lstlisting}
\begin{figure}[H]
  \begin{center}
  \includegraphics[width=0.8\linewidth]{petal_width_scatter}
  \caption{Scatterplot of a petal width column}
  \end{center}
\end{figure}

\subsection{Multivariate non-graphical EDA}
\label{sec:multi-non}

Multivariate non-graphical EDA shows the relationship between two variables in form of either cross-tabulation (categorical data) or statistics (quantitative data).

To get multiple columns from a DataFrame we can give it an array of column names or numbers, or ask for all the columns in a given range, specified by two numbers. The response to any of these messages will also be a DataFrame, and the order of columns will be the same as requested.

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
data columns: #(petal_length sepal_width sepal_length).
data columnsAt: #(3 2 1).
data columnsFrom: 3 to: 1.
\end{lstlisting}

When asked for a value of certain statistical measure, DataFrame will answer a DataSeries of the same size as one of its rows, with keys equal to the column names and the values of that statistics applied to the corresponding columns. For example, we can ask the Iris DataFrame for its average

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
data average.
\end{lstlisting}

The response will be a DataSeries storing with average values of every quantitative column of a DataFrame and \texttt{Float nan} for all the categorical ones.

\begin{lstlisting}[basicstyle=\small]
sepal_length |     5.843
sepal_width  |     3.054
petal_length |     3.759
petal_width  |     1.199
species      | Float nan
\end{lstlisting}

\subsection{Multivariate graphical EDA}
\label{sec:multi-graph}

According to \cite{Seltman}, the most commonly used technique of graphical EDA is a grouped barplot with each group representing one level of one of the variables and each bar within a group representing the levels of the other variable. Another useful visualization is a side-by-side boxplot. It is considered the best graphical EDA technique for examining the relationship between a categorical variable and a quantitative variable. These two visualizations are not present in the current release, but are to be integrated in a future version of DataFrame.

We can ask DataFrame to show us the barplots of all the quantitative columns, aligned on the same axis. This can be useful for comparing variables among themselves.

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
data boxplot.
\end{lstlisting}
\begin{figure}[H]
  \begin{center}
  \includegraphics[width=0.9\linewidth]{boxplot}
  \caption{Boxplots of all the quantitative columns of a data frame. Left to right: sepal length, sepal width, petal length, petal width}
  \end{center}
\end{figure}

% Let's look at the scatterplot of two variables representing the width and length of a sepal. To do that we ask our DataFrame to give us specific columns, in our case, \texttt{sepal\_width} and \texttt{sepal\_length}, then we ask these columns (the result will be another DataFrame) to visualize themselves.
%
% \begin{lstlisting}[basicstyle=\small,language=Smalltalk]
% (data columns: #(sepal_width sepal_length)) scatterplot.
% \end{lstlisting}
%
% \begin{figure}[H]
%   \begin{center}
%   \includegraphics[width=0.9\linewidth]{sepal_wl_scatter}
%   \end{center}
% \end{figure}

We can also ask for a matrix that contains all the pairwise scatterplots of the variables on a single page in a matrix format. Scatterplot matrix is a complex visualization which allows us to roughly determine if we have a linear correlation between multiple variables.

\begin{lstlisting}[basicstyle=\small,language=Smalltalk]
data scatterplotMatrixOn: #species.
\end{lstlisting}

Quantitative variables are plotted against each other, and the values of a given categorical variable are mapped with colors. The column names are written in a diagonal line from top left to bottom right.

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=\linewidth]{scatterplot_matrix}
  \caption{Scatterplot matrix of Iris dataset}
  \end{center}
\end{figure}

% \section{Discussion}
% \label{sec:discussion}
%
% Discussion of actual solution \emph{vs.} initial constraints from
% \ref{sec:problem}. Explain the space of the solution, why we made it this way.
%
% Evaluation of the solution. How does the solution meet the criteria? Where
% does it succeed or fails...


% \section{Related Works}
% \label{sec:related}
%
% Other solutions in the domain, and a real comparison of our contribution with
% solutions from other people.

\section{Conclusion \& Future work}
\label{sec:conclusion}
We have introduced the \texttt{DataFrame} and \texttt{DataSeries} collections in the Pharo Collection hierarchy. These classes are specifically designed for working with structured data. We have demonstrated how these classes can be used for descriptive statistics and exploratory data analysis, such as getting the summary of a dataset, detect mistakes, determine relations, and select appropriate models for further investigation. We detailed the implementation trade-offs that we are currently facing in our implementation for Pharo and exemplified our contribution through an \textit{Exploratory Data Analysis} session for the Iris Dataset, that included both univariate and multivariate (graphical and non-graphical) analyses. 

In terms of future work we intend to evolve our API and implementation by investigating more complicated scenarios and datasets, extend the usage of traits in our design and finally add support for more advanced protocols for data wrangling, aggregation and grouping.
%
% At the time of writing this paper DataFrame is capable of...
% However, a lot of functionality is still missing. For example, we need tools for
% \begin{itemize}
%   \item data wrangling
%   \item data aggregation and grouping
% \end{itemize}

% \section{Conclusion}


% In this paper, we \textsf{looked}\xspace at problem P with this context and these
% constraints. We proposed solution S. It has such good points and such not so
% good ones. Now we could do this or that.


% \section*{macro example}
%
% \ct{look at it this is code }
% \begin{code}{}
% Class>>nknkjbkjbkjb
%     \{| grgr | 
%     grgrgrgg 
%     a := 
% \end{code}

% \subsection*{Acknowledgements} This work was supported by Ministry of Higher Education and Research, Nord-Pas de Calais Regional Council, FEDER through the 'Contrat de
% Projets Etat Region (CPER) 2007-2013',  the Cutter ANR project, ANR-10-BLAN-0219 and the MEALS Marie Curie Actions program FP7-PEOPLE-2011-
% IRSES MEALS (no. 295261). 

% \bibliographystyle{plain}
% \bibliography{foo.bib}

% \appendix
% 
% \section{Lots of Furry Technical Details}

\bibliographystyle{alpha}
\bibliography{pharoeda}

%\bibliography{rmod,others}
%\end{multicols}
\end{document}

%%% Local Variables: 
%%% coding: utf-8
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-PDF-mode: t
%%% End:
