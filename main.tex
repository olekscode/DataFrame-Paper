\documentclass[sigplan]{acmart}

\usepackage[T1]{fontenc}
\usepackage{lmodern}
\usepackage[utf8]{inputenc}

%% Some recommended packages.
\usepackage{booktabs}   %% For formal tables:
                        %% http://ctan.org/pkg/booktabs
\usepackage{subcaption} %% For complex figures with subfigures/subcaptions
% DOI
\acmDOI{10.475/123_4}

% ISBN
\acmISBN{123-4567-24-567/08/06}

%Conference
\acmConference[IWST'17]{International Workshop on Smalltalk Technologies}{ESUG 2017}{Maribor, Slovenia 4-8 September 2017} 
%\acmYear{1997}
\copyrightyear{2017}

%\acmPrice{15.00}

%\acmBadgeL[http://ctuning.org/ae/ppopp2016.html]{ae-logo}
%\acmBadgeR[http://ctuning.org/ae/ppopp2016.html]{ae-logo}



\usepackage[T1]{fontenc} %%%key to get copy and paste for the code!
\usepackage[utf8]{inputenc} %%% to support copy and paste with accents for frnehc stuff
%\usepackage{times}
%\usepackage[scaled=0.85]{helvet}
\usepackage{graphicx}
\usepackage{ifthen}
\usepackage{xspace}
\usepackage{alltt}
\usepackage{latexsym}
\usepackage{url}            
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{stmaryrd}
\usepackage{enumerate}
%\usepackage{cite}
% \usepackage[pdftex,colorlinks=true,pdfstartview=FitV,linkcolor=blue,citecolor=blue,urlcolor=blue]{hyperref}
%\usepackage{xspace}
%\newcommand*{\eg}{e.g.\@\xspace}
%\newcommand*{\ie}{i.e.\@\xspace}

\makeatletter
\newcommand*{\etc}{%
    \@ifnextchar{.}%
        {etc}%
        {etc.\@\xspace}%
}
\makeatother

\usepackage{float}
% \usepackage{xcolor}
% \usepackage{listings}
% \usepackage{highlight}

%\usepackage{multicol}
\usepackage{url}
\usepackage{multicol}

% \usepackage
% [
%   left=20mm,
%   right=20mm,
%   top=30mm,
%   bottom=30mm,
%   bindingoffset=10mm
%   % use vmargin=2cm to make vertical margins equal to 2cm.
%   % use hmargin=3cm to make horizontal margins equal to 3cm.
%   % use margin=3cm to make all margins  equal to 3cm.
% ] {geometry}

\input{macros}

%\setlength\parindent{0pt}

% \date{}

\input{smalltalkEnv}

\begin{document}
\title{Towards Exploratory Data Analysis for Pharo}
% \titlenote{Produces the permission block, and
%   copyright information}
% \subtitle{Extended Abstract}
% \subtitlenote{The full version of the author's guide is available as
%   \texttt{acmart.pdf} document}

\author{Oleksandr Zaytsev}
%\authornote{Dr.~Trovato insisted his name be first.}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{Ivan Franko National University\\
  Faculty of Applied Mathematics and Informatics, Ukraine}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin}
  % \state{Ohio}
  % \postcode{43017-6221}
}
\email{olk.zaytsev@gmail.com}

\author{Nick Papoulias}
%\authornote{Dr.~Trovato insisted his name be first.}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{UMMISCO IRD France Nord, Bondy\\
  Sorbonne Universités UPMC, Univ. Paris 06, France\\}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin}
  % \state{Ohio}
  % \postcode{43017-6221}
}
\email{npapoylias@gmail.com}


\author{Serge Stinckwich}
%\authornote{Dr.~Trovato insisted his name be first.}
%\orcid{1234-5678-9012}
\affiliation{%
  \institution{UMMISCO IRD France Nord, Bondy\\
  Sorbonne Universités UPMC, Univ. Paris 06, France\\
   Université de Caen Normandie, Caen}
  % \streetaddress{P.O. Box 1212}
  % \city{Dublin}
  % \state{Ohio}
  % \postcode{43017-6221}
}
\email{serge.stinckwich@ird.fr}

%\title{Title that Describes the Contribution that Solves a Problem}
%\author{Oleksandr Zaytsev}
%\date{\today}
%\maketitle
\renewcommand{\shortauthors}{O. Zaytsev et al.}
\begin{abstract}
% In this context...
% We consider this problem P...
% P is a problem because...
% We propose this solution...
% Our solution solves P in such and such way.

Data analysis and visualizations techniques (such as split-apply-combine) make extensive use of associative tabular data-structures that are cumbersome to use with common aggregation APIs (for arrays, lists or dictionaries). In these cases a fluent API for querying associative tabular data (like the ones provided by Pandas, Mathematica or LINQ) is more appropriate for interactive exploration environments. In Smalltalk despite the fact that many important analysis tools are already present (for \eg in the PolyMath library), we are still missing this essential part of the data science toolkit. These specialized data structures for tabular data sets can provide us with a simple and powerful API for summarizing, cleaning, and manipulating a wealth of data-sources that are currently cumbersome to use. In this paper we introduce the \texttt{DataFrame} and \texttt{DataSeries} collections - that are specifically designed for working with structured data. We demonstrate how these tools can be used for descriptive statistics and exploratory data analysis - the critical first step of data analysis which allows us to get the summary of a data set, detect mistakes, determine the relations, and select the appropriate model for further confirmatory analysis. We then detail the implementation trade-offs that we are currently facing in our implementation for Pharo and discuss future perspectives.
\end{abstract}


\begin{CCSXML}
<ccs2012>
 <concept>
  <concept_id>10010520.10010553.10010562</concept_id>
  <concept_desc>Computer systems organization~Embedded systems</concept_desc>
  <concept_significance>500</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010575.10010755</concept_id>
  <concept_desc>Computer systems organization~Redundancy</concept_desc>
  <concept_significance>300</concept_significance>
 </concept>
 <concept>
  <concept_id>10010520.10010553.10010554</concept_id>
  <concept_desc>Computer systems organization~Robotics</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
 <concept>
  <concept_id>10003033.10003083.10003095</concept_id>
  <concept_desc>Networks~Network reliability</concept_desc>
  <concept_significance>100</concept_significance>
 </concept>
</ccs2012>
\end{CCSXML}

\ccsdesc[500]{Computer systems organization~Embedded systems}
\ccsdesc[300]{Computer systems organization~Redundancy}
\ccsdesc{Computer systems organization~Robotics}
\ccsdesc[100]{Networks~Network reliability}


\keywords{Tabular Data-structures, Fluent APIs, Exploratory Data Analysis,
Live Environments}

\maketitle

%\begin{multicols}{2}
\section{Introduction}
\label{sec:intro}

% Context
%
% Problem
%
% Known tracks for \sd{solutions}
% 	here you want to show that you are not an idiot not knowing what have been around
%
% What our solution is \ct{Set} and \ct{OrderedCollection} (so that the reader knows where the paper is going)
%
% Contribution of the paper

The simplicity and power of Smalltalk combined with the live environment of Pharo creates a productive combination for data analysis. 
% The advanced debugging and inspecting tools together with the library for agile visualizations allow us to communicate and play with every object in our system. This includes all the logical components of both data and the algorithm.
Provided the proper tools and open source libraries for machine learning, statistics, and optimization, Pharo can become both a powerful tool for professional data analysts, and a simple environment for everyone who wants to experiment with a simple data set. Many important tools and algorithms are already implemented in libraries such as PolyMath, but we are still missing essential data-structures and a fluent API for advanced data-analysis techniques. To overcome this problem we introduce in this paper the \texttt{DataFrame} and \texttt{DataSeries} collections for working with structured data, and through several examples demonstrate how these tools can be used for descriptive statistics and exploratory data analysis allowing us to get the summaries of data sets, detect mistakes, determine relations, and select the appropriate models for further analysis.

Our work is motivated by popular data-analysis techniques (such as split-apply-combine \cite{wickham2011split} and collection pipelines \cite{Fowler15}) as well as from dedicated data-structure and APIs for data-analysis environments for Python \cite{McKinney}, R \cite{team2000r}, Mathematica \cite{wolfram1999mathematica} and the .Net platform (through the LINQ embedded query language \cite{meijer2006linq}). Nevertheless our primary inspiration is drawn form Smalltalk itself and can be traced back to the extensive coverage of the Smalltalk-80 book on Collections \cite{Goldberg} (with over four chapters dedicated to the usage and design of the Collection hierarchy). As well as from more recent work from our community that refined the implementation of collections using traits \cite{black2003applying, bourgois2010bloc, scharli2003traits} and promises \cite{Alcocer16}.

To facilitate our readers, we should note here that all figures in this paper are DataFrame visualizations that are created with Roassal2 and can be reproduced by the steps described in Section \ref{sec:dataframe}. The rest of this paper is structured as follows: Section \ref{sec:eda} provides a brief introduction into explanatory data analysis, answering the following question: \textit{What is EDA good for and how to do it right ?}. It also provides basic knowledge about statistics and data analysis, such as: statistical variables, types of variables \etc. Then Section \ref{sec:dataframe} details the new DataSeries and DataFrame Collections for structured data. Section \ref{sec:contribution} gives a step-by-step example of how to perform EDA on the well-known Iris data set using the new collections and API. Finally Section \ref{sec:conclusion} concludes the paper and discusses implementation trade-offs as well as future perspectives.




% \section{Problem Description}
% \label{sec:problem}

% Context, exposed with the \textbf{most precise terms possible} (don't open
% unwanted doors for the reader)
%
% Probably set the vocabulary before to cut any misinterpretation
%
% Constraints that influenced the solution (because the solution is not
% universal) \emph{e.g.} our requirements for a solution, possibly not all
% satisfied. They should be sound and believable. Analysis of the criteria.
% Imagine that you are another guy having this problem do the constraint
% matches yours so that you could apply the solution
%
% Problem
%
% Factual solution tracks, to position...
% Our solution in a nutshell.

\section{Exploratory Data Analysis}
\label{sec:eda}
\textit{Exploratory data analysis (EDA)} is an approach for analyzing data sets to summarize their main characteristics. It allows us to make some sense of the data by visualizing it and exploring its statistical properties. According to Howard J. Seltmanan, any method for looking at data that does not include formal statistical modeling and inference falls under the term exploratory data analysis \cite{Seltman}. It is an important first step of data analysis which helps us to select the model that we will be fitting to the data during the following steps of confirmatory data analysis.

EDA can be particularly useful for uncovering the underlying structure of a dataset, detecting outliers and anomalies, determining relationships among the explanatory variables, assessing the direction and rough size of relationships between explanatory and outcome variables, and selecting appropriate models for further analysis\cite{eStats}.

Depending on the number of variables involved and the input/output abstractions used, the techniques of exploratory data analysis can be classified into four categories: univariate non-graphical (Sub-section \ref{sec:uni-non}), univariate graphical (Sub-section \ref{sec:uni-graph}), multivariate non-graphical (Sub-section \ref{sec:multi-non}) and multivariate graphical (Sub-section \ref{sec:multi-graph}).

\section{DataSeries and DataFrame}
\label{sec:dataframe}
DataFrame and DataSeries are the high-level data structures with which we intent to make data analysis in Pharo fast and easy.%\cite{McKinney}

They can be loaded into a Pharo image with the following Metacello script:

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
Metacello new
    baseline: 'DataFrame';
    repository: 'github://PolyMathOrg/DataFrame';
    load.
\end{lstlisting}

\textbf{A DataSeries} can be seen as an Ordered Collection that combines the properties of an Array and a Dictionary, while extending the functionality of both. Every DataSeries has a name and contains an array of data mapped to a corresponding array of keys (that are used as index values).

The easiest way of to create a series is by converting an array: 

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
series := #(a b c) asDataSeries.
\end{lstlisting}

The keys will be automatically set to the numeric sequence of the array indexes, which can be described as an interval (1 to: n), where n is the size of array. The name of the series at this point will remain empty. Both the name and the keys of a DataSeries can be changed later, as follows:

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
series name: 'letters'.
series keys: #(a1 a2 a3).
\end{lstlisting}

\textbf{A DataFrame} is a tabular data structure that can be seen as an ordered collection of columns. It works like a spreadsheet or a relational database with one row per subject and one column for each subject identifier, outcome variable, explanatory variable \etc. A DataFrame has both row and column indices which can be changed if needed. The important feature of a DataFrame is that whenever we ask for a specific row or column, it responds with a DataSeries object that preserves the same indexing.

A simple DataFrame can be created from an array of rows or columns.

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
df:=DataFrame 
     rows: #((John 25 true)(Jane 21 false)).
df:=DataFrame 
     columns: #((John Jane)(25 21)(true false)).
\end{lstlisting}

Those two line produce exactly the same DataFrame. Once again, the names (key values) of both rows and columns were not explicitly specified, so by default they will set to numerical indices: \#(1 2) for rows and \#(1 2 3) for columns. These names can be changed later:

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
df columnNames: #(Name Age IsMarried).
\end{lstlisting}

\subsection{Discussion: DataFrame Internals}

Before continuing with a detailed example on how to use these classes 
for data exploration, we would like to share our thoughts on their implementation. 

Our goal for these classes is to fully encapsulate their internal structure so that it is both easy to experiment with optimized  representations, but also allow for a polymorphic fluent API that can be implemented separately without breaking existing code. 

This is why we are evolving the API and the implementation according to concrete needs we are facing while analyzing specific data-sets (like the Iris or Housing data-sets that we will shortly discuss). We are currently in the fourth iteration of the model, since there are many trade-offs to consider, between for \eg using inheritance, instance composition and/or trait composition for the implementation. Even inheriting directly for \eg from Array, Dictionary or OrdreredDictionary comes with its own set of trade-offs. In the future we might consider reusing the internal structure of the pandas DataFrame in which the data is stored as one or more two-dimensional blocks rather than a collection of one-dimensional arrays \cite{McKinney}. 

Finally functionalities such as the head/tail protocol or the protocol allowing us to change the keys of a data structure are all currently implemented as traits. In future iterations, we expect to extend the use of traits to include the data visualization and aggregation protocols, that should be shared among the DataSeries and DataFrame classes. 
%
% Internally DataSeries and DataFrame are implemented as subclasses of OrderedDictionary. This way they support the key/value/association interface of a Dictionary and preserves the key insertion order. This is the fourth iteration of a model. Previously the data structures were implemented as subclasses of Dictionary or Array. Both approaches have proved to be ineffective, because an Array can not have variable size and Dictionary stores its elements in an unordered manner and uses the hashing technique to locate them\cite{Goldberg}. The model is evolving according to the needs, so while this implementation works fine on small datasets (like Iris or Housing data), it might prove to be useless when dealing with massive datasets. For that reason in future iterations we might consider reusing the internal structure of pandas DataFrame in which the data is stored as one or more two-dimensional blocks rather than a collection of one-dimensional arrays\cite{McKinney}.
% %
% % Another important thing about the implementation of DataFrame collections is the use of composition instead of inheritance. The problem is that DataSeries (one-dimensional) and DataFrame (two-dimensional) are classes of different nature that may require the entirely different intertal implementation. Nevertheless, we want them to be polymorphic and respond to the same messages (many of these messges will also have the same implementation). One way of achieving polymorphic behavior and code reuse without using inheritance is the composition reuse principle. In Pharo this can be done using the Traits instrument.
%
% At this point the head/tail functionality and the method that allows us to change keys of a data structure are implemented as traits. In future iterations, the data visualization, aggregation, and many other maethods that should be shared by DataSeries and DataFrame are also likely to be implemented as traits.

% Here is an example of how both DataSeries and DataFrame respond to a \texttt{head} message: "take first 5 elements from a collection, construct a collection of the same type with these elements, and return it". For a DataFrame these 5 elements will be rows of data (regardless of the internal implementation, the \texttt{at:} message should return a row as a single observation and a logical unit of a DataFrame). For DataSeries, these will be the atomic values.

%\section{Proposed Solution}%
\section{Exploring Iris Data Set}
\label{sec:contribution}
%
% Free form, variable number of sections, technical details.
%
% But in general do not mix solution and discussions/possible variation
% let that for discussion

We can start by loading iris data set from a CSV file.

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
data := DataFrame fromCsv: '/path/to/iris.csv'.
\end{lstlisting}

DataFrame comes with a built-in collection of data sets that are widely used as examples for data analysis and machine learning problems. Iris is among them, so an alternative way of loading it would be simply

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
data := DataFrame loadIris.
\end{lstlisting}

Now let's take a look at the first and the last 5 entries in our table. These slices are called \textit{head} and \textit{tail} of a data frame.
\begin{code}{}
data head.
data tail.
\end{code}
The output will be the following table

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
(5.1 3.5 1.4 0.2 #setosa)
(4.9 3 1.4 0.2 #setosa)
(4.7 3.2 1.3 0.2 #setosa)
(4.6 3.1 1.5 0.2 #setosa)
(5 3.6 1.4 0.2 #setosa)
\end{lstlisting}

\subsection{Univariate non-graphical EDA}
\label{sec:uni-non}
To access a single variable we ask a data frame for a specific column, using its name or number. The result will be a DataSeries object.

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
series := data column: #sepal_width.
series := data columnAt: 1.
\end{lstlisting}

What we can do with a column depends on a type of statistical variable it represents. The best univariate non-graphical EDA for categorical data is a simple tabulation of the frequency of each category\cite{Seltman}. If the data is quantitative, we can ...
\texttt{min, max, range, average, median, mode, stdev, variance}

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
series average. 
series stdev.
\end{lstlisting}

\subsection{Univariate graphical EDA}
\label{sec:uni-graph}

The only graphical technique that can be used for a categorical variable is histogram - a barplot where the hight of each bar represents the proportion (count/total count) of cases for a range of values.
Histogram is the only graphical technique that can be used for a categorical variable.

\begin{lstlisting}[basicstyle=\small,numbers=left,language=Smalltalk,numberstyle=\tiny]
var := data column: #species.
var histogram.
\end{lstlisting}

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=0.75\linewidth]{species_bar}
  \end{center}
\end{figure}

\subsection{Multivariate non-graphical EDA}
\label{sec:multi-non}

Multivariate non-graphical EDA shows the relationship between two variables in form of either cross-tabulation (categorical data) or statistics (quantitative data).

\subsection{Multivariate graphical EDA}
\label{sec:multi-graph}

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=0.75\linewidth]{boxplot}
  \end{center}
\end{figure}

Let's look at the scatterplot of two statistical variables representing the width and length of a sepal. To do that we ask our DataFrame to give us specific columns, in our case, \texttt{sepal\_width} and \texttt{sepal\_length}, then we ask these columns (the result will be another DataFrame) to visualize themselves.

\begin{code}{}
vars := data columns: #(sepal_width sepal_length).
vars scatterplot.
\end{code}

\begin{figure}[H]
  \begin{center}
  \includegraphics[width=0.75\linewidth]{sepal_wl_scatter}
  \end{center}
\end{figure}

% \section{Discussion}
% \label{sec:discussion}
%
% Discussion of actual solution \emph{vs.} initial constraints from
% \ref{sec:problem}. Explain the space of the solution, why we made it this way.
%
% Evaluation of the solution. How does the solution meet the criteria? Where
% does it succeed or fails...


% \section{Related Works}
% \label{sec:related}
%
% Other solutions in the domain, and a real comparison of our contribution with
% solutions from other people.

\section{Conclusion \& Future work}
\label{sec:conclusion}
At the time of writing this paper DataFrame is capable of...
However, a lot of functionality is still missing. For example, we need tools for
\begin{itemize}
  \item data wrangling
  \item data aggregation and grouping
\end{itemize}

% \section{Conclusion}


% In this paper, we \textsf{looked}\xspace at problem P with this context and these
% constraints. We proposed solution S. It has such good points and such not so
% good ones. Now we could do this or that.


% \section*{macro example}
%
% \ct{look at it this is code }
% \begin{code}{}
% Class>>nknkjbkjbkjb
%     \{| grgr | 
%     grgrgrgg 
%     a := 
% \end{code}

% \subsection*{Acknowledgements} This work was supported by Ministry of Higher Education and Research, Nord-Pas de Calais Regional Council, FEDER through the 'Contrat de
% Projets Etat Region (CPER) 2007-2013',  the Cutter ANR project, ANR-10-BLAN-0219 and the MEALS Marie Curie Actions program FP7-PEOPLE-2011-
% IRSES MEALS (no. 295261). 

% \bibliographystyle{plain}
% \bibliography{foo.bib}

% \appendix
% 
% \section{Lots of Furry Technical Details}

\bibliographystyle{plain}
\bibliography{pharoeda}

%\bibliography{rmod,others}
%\end{multicols}
\end{document}

%%% Local Variables: 
%%% coding: utf-8
%%% mode: latex
%%% TeX-master: "main"
%%% TeX-PDF-mode: t
%%% End:
